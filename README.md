<p align="center">
  <img src="https://readme-typing-svg.herokuapp.com?font=Fira+Code&size=28&duration=3000&pause=800&color=4B8BBE&center=true&vCenter=true&width=800&lines=Bijal+Bharadva;Offline+Reinforcement+Learning;Machine+Unlearning+Researcher;Building+Systems+That+Forget+Responsibly" />
</p>

---

<p align="center">
  <b>Machine Learning Engineer ‚Ä¢ Researcher ‚Ä¢ Systems Builder</b><br>
  MS Computer Science ‚Äî San Diego State University
</p>

---
# Current Research

<p align="center">
<b>Adaptive Learning Systems Under Distributional Shift</b>
</p>

---

## <img src="https://img.icons8.com/ios-filled/20/000000/artificial-intelligence.png"/> Vision-Language Models  
### Open-World & Continual Learning

<img src="https://img.shields.io/badge/Focus-OOD_Detection-black?style=flat-square"/>
<img src="https://img.shields.io/badge/Setting-Open_World-black?style=flat-square"/>
<img src="https://img.shields.io/badge/Paradigm-Class_Incremental-black?style=flat-square"/>

Models deployed in real environments must handle evolving tasks and unknown classes.

**Core Question**

> How does OOD detection learned in earlier tasks  
> influence OOD detection performance in later tasks?

**Current Investigation**
- OOD boundary drift across task streams  
- Calibration stability under incremental updates  
- Representation geometry evolution  
- Formalizing inter-task knowledge transfer  
- Quantifying positive vs negative uncertainty transfer  

---

## <img src="https://img.icons8.com/ios-filled/20/000000/decision.png"/> CURe  
### Conservative Unlearning in Offline RL

<img src="https://img.shields.io/badge/Method-Gradient_Influence-black?style=flat-square"/>
<img src="https://img.shields.io/badge/Algorithm-CQL-black?style=flat-square"/>
<img src="https://img.shields.io/badge/Goal-Stable_Unlearning-black?style=flat-square"/>

Experience accumulates in RL systems.  
Some trajectories must be removed.

**Core Question**

> Can trajectory influence be removed  
> without retraining the entire policy?

**Approach**
- Cosine similarity between forget and retain TD gradients  
- Influence-aware soft gating  
- Conservative Q regularization  

Result: Efficient unlearning with stable returns.

---

## <img src="https://img.icons8.com/ios-filled/20/000000/privacy.png"/> PateGAIL++

### Privacy-Aware Imitation Learning

<img src="https://img.shields.io/badge/Constraint-Differential_Privacy-black?style=flat-square"/>
<img src="https://img.shields.io/badge/Domain-Mobility_Modeling-black?style=flat-square"/>
<img src="https://img.shields.io/badge/Objective-Utility_Preservation-black?style=flat-square"/>

Learning from behavioral trajectories under strict privacy guarantees.

**Central Question**

> How much policy utility can be preserved  
> under strong privacy constraints?

(ICLR 2026 ‚Äî Accepted)

---

<p align="center">
<b>Unifying Theme</b><br>
Stability ¬∑ Uncertainty ¬∑ Controlled Adaptation
</p>

# ‚öô Technical Stack

### üíª Languages
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![SQL](https://img.shields.io/badge/SQL-336791?style=for-the-badge&logo=postgresql&logoColor=white)
![R](https://img.shields.io/badge/R-276DC3?style=for-the-badge&logo=r&logoColor=white)
![C++](https://img.shields.io/badge/C++-00599C?style=for-the-badge&logo=cplusplus&logoColor=white)

---

### üß† ML & Research
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)
![HuggingFace](https://img.shields.io/badge/HuggingFace-FFCC00?style=for-the-badge&logo=huggingface&logoColor=black)
![Scikit-Learn](https://img.shields.io/badge/ScikitLearn-F7931E?style=for-the-badge&logo=scikitlearn&logoColor=white)

---

### üéØ RL & Unlearning
![Offline RL](https://img.shields.io/badge/Offline_RL-2E8B57?style=for-the-badge)
![CQL](https://img.shields.io/badge/CQL-4B0082?style=for-the-badge)
![Imitation Learning](https://img.shields.io/badge/Imitation_Learning-708090?style=for-the-badge)
![Machine Unlearning](https://img.shields.io/badge/Machine_Unlearning-8B0000?style=for-the-badge)

---

### ‚òÅ Infra & Optimization
![AWS](https://img.shields.io/badge/AWS-232F3E?style=for-the-badge&logo=amazonaws&logoColor=white)
![GCP](https://img.shields.io/badge/GCP-4285F4?style=for-the-badge&logo=googlecloud&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![MLflow](https://img.shields.io/badge/MLflow-0194E2?style=for-the-badge&logo=mlflow&logoColor=white)
![TensorRT](https://img.shields.io/badge/TensorRT-76B900?style=for-the-badge&logo=nvidia&logoColor=black)

---

# üì´ Connect

<p align="center">
  <a href="mailto:bbharadva1752@sdsu.edu">
    <img src="https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&logo=gmail&logoColor=white"/>
  </a>
  <a href="https://www.linkedin.com/in/bijal-bharadva/">
    <img src="https://img.shields.io/badge/LinkedIn-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white"/>
  </a>
  <a href="https://github.com/Bijal1121/">
    <img src="https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white"/>
  </a>
</p>

---

<p align="center">
  <i>Focused on stability, influence, and responsible learning dynamics.</i>
</p>
